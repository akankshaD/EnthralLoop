{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import keras\n",
    "import numpy as np\n",
    "#import nltk\n",
    "import re\n",
    "#from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "#from keras.optimizers import Adadelta\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract list of genres for every record and pre-process it.\n",
    "def extractGenres(row):\n",
    "    json_dict_list = json.loads(row.genres)\n",
    "    genres = [d['name'].lower() for d in json_dict_list if 'name' in d]\n",
    "    return \" \".join(genres)\n",
    "\n",
    "# Read dataset file and extract features and labels from it.\n",
    "def readFile(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    X = []\n",
    "    for row in df.itertuples():\n",
    "        plot = str(row.overview)\n",
    "        plot_chars = re.sub(\"[^a-zA-Z]\", \" \", plot)\n",
    "        X.append(plot_chars)\n",
    "    Y = [extractGenres(row).rstrip() for row in df.itertuples()]\n",
    "    return X,Y\n",
    "\n",
    "# Tokenize features and labels into one-hot matrics\n",
    "def tokenize(X, Y, XTokenizer, YTokenizer):\n",
    "    \n",
    "    plot_matrix = XTokenizer.texts_to_matrix(X)\n",
    "    genre_matrix = YTokenizer.texts_to_matrix(Y)\n",
    "    \n",
    "    return plot_matrix, genre_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4803, 20937)\n",
      "(4803, 23)\n"
     ]
    }
   ],
   "source": [
    "file_path = 'tmdb_5000_movies.csv'\n",
    "X, Y = readFile(file_path)\n",
    "\n",
    "XTokenizer = Tokenizer(filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
    "YTokenizer = Tokenizer(filters = ' ', lower = True,)\n",
    "    \n",
    "XTokenizer.fit_on_texts(X)\n",
    "YTokenizer.fit_on_texts(Y)\n",
    "\n",
    "plot_matrix, genre_matrix = tokenize(X, Y, XTokenizer, YTokenizer)\n",
    "\n",
    "input_size = plot_matrix.shape[1]\n",
    "output_size = genre_matrix.shape[1]\n",
    "\n",
    "print(plot_matrix.shape)\n",
    "print(genre_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.9\n",
    "split_index = int(plot_matrix.shape[0] * split_ratio)\n",
    "\n",
    "X_train, X_test = np.array(plot_matrix[: split_index]), np.array(plot_matrix[split_index : ])\n",
    "Y_train, Y_test = np.array(genre_matrix[: split_index]), np.array(genre_matrix[split_index : ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 512)               10720256  \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 23)                2967      \n",
      "=================================================================\n",
      "Total params: 10,887,447\n",
      "Trainable params: 10,887,447\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation = 'relu', input_dim = input_size))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(output_size, activation = 'sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#adadelta = Adadelta(lr =1.0, rho = 0.95, decay = 0.0)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics = ['top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=None, monitor = 'val_loss', mode = 'min', verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4322/4322 [==============================] - 11s 3ms/step - loss: 7.3980 - top_k_categorical_accuracy: 0.8839\n",
      "Epoch 2/10\n",
      "4322/4322 [==============================] - 8s 2ms/step - loss: 6.4873 - top_k_categorical_accuracy: 0.9165\n",
      "Epoch 3/10\n",
      "4322/4322 [==============================] - 11s 2ms/step - loss: 5.6878 - top_k_categorical_accuracy: 0.9128\n",
      "Epoch 4/10\n",
      "4322/4322 [==============================] - 10s 2ms/step - loss: 5.1108 - top_k_categorical_accuracy: 0.9264\n",
      "Epoch 5/10\n",
      "4322/4322 [==============================] - 10s 2ms/step - loss: 4.7297 - top_k_categorical_accuracy: 0.9438\n",
      "Epoch 6/10\n",
      "4322/4322 [==============================] - 9s 2ms/step - loss: 4.4738 - top_k_categorical_accuracy: 0.9544\n",
      "Epoch 7/10\n",
      "4322/4322 [==============================] - 9s 2ms/step - loss: 4.2086 - top_k_categorical_accuracy: 0.9660\n",
      "Epoch 8/10\n",
      "4322/4322 [==============================] - 9s 2ms/step - loss: 4.0594 - top_k_categorical_accuracy: 0.9722\n",
      "Epoch 9/10\n",
      "4322/4322 [==============================] - 9s 2ms/step - loss: 3.9187 - top_k_categorical_accuracy: 0.9803\n",
      "Epoch 10/10\n",
      "4322/4322 [==============================] - 8s 2ms/step - loss: 3.8264 - top_k_categorical_accuracy: 0.9792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4e3b8ba8>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs = 10, batch_size = 128, verbose = 1, callbacks= [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy [5.595424637229428, 0.760914760914761]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose = 0)\n",
    "print(\"Accuracy %s\"% score)\n",
    "model.save('models/el_model_77_acc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessTestData(comments):\n",
    "    clean_comments = []\n",
    "    for comment in comments:\n",
    "        comment_chars = re.sub(\"[^a-zA-Z]\", \" \", comment)\n",
    "        clean_comments.append(comment_chars.rstrip())\n",
    "    return clean_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = ['I would have been the first, but i did not have a gun. not really though, zac snyder is just a clown.',\n",
    "            'I love my dog',\n",
    "            'I believe in humanity',\n",
    "            'I feel suicidal today.']\n",
    "comments = preprocessTestData(comments)\n",
    "comments_test = XTokenizer.texts_to_matrix(comments)\n",
    "y_probs = model.predict(np.array(comments_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 3\n",
    "prob_to_indices = lambda y_prob, k: np.argpartition(y_prob, -k)[-k : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_to_index_dict = YTokenizer.word_index\n",
    "index_to_genre = [0]*output_size\n",
    "genre_to_index_dict[''] = 0\n",
    "for k, v in genre_to_index_dict.items():\n",
    "    index_to_genre[v] = k\n",
    "indices_to_genres = lambda ind : [index_to_genre[i] for i in ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_genres = [indices_to_genres(prob_to_indices(p, top_k)) for p in y_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drama', 'crime', 'thriller']\n",
      "['romance', 'comedy', 'drama']\n",
      "['action', 'thriller', 'drama']\n",
      "['crime', 'drama', 'thriller']\n"
     ]
    }
   ],
   "source": [
    "for out in output_genres:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe = pd.read_csv(file_path)\n",
    "genre_to_movies_dict = {} # dictionary mapping each genre to its corresponding list of movies.\n",
    "movies_popularity_dict = {} # storing movies with their corresponding calculated scores.\n",
    "index_to_movie_list = [] # mapping index to corresponding movie.\n",
    "\n",
    "# Parameters for calculating weighted review:\n",
    "C = 6.9 # mean vote across whole report\n",
    "M = 3000 # minimum votes required to be listed in Top 250 by IMDB.\n",
    "for row in dframe.itertuples():\n",
    "    V = row.vote_count\n",
    "    R = row.vote_average\n",
    "    WR = (V/(V+M))*R + (M/(V+M))*C\n",
    "    movies_popularity_dict[row.title] = WR\n",
    "    index_to_movie_list.append(row.title)\n",
    "    genre_map = json.loads(row.genres)\n",
    "    genres = [d['name'].lower() for d in genre_map if 'name' in d]\n",
    "    for genre in genres:\n",
    "        if not genre in genre_to_movies_dict:\n",
    "            genre_to_movies_dict[genre] = []\n",
    "        genre_to_movies_dict[genre].append(row.title)\n",
    "movie_to_index = {}\n",
    "for i, movie in enumerate(movies_popularity_dict):\n",
    "    movie_to_index[movie] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genres_to_top_k_movies(genres, k=5):\n",
    "    counter = [0]*len(movies_popularity_dict)\n",
    "    for genre in genres:\n",
    "        for movie in genre_to_movies_dict[genre]:\n",
    "            counter[movie_to_index[movie]] += 1\n",
    "    count_pop_tuple_list = []\n",
    "    for i, count in enumerate(counter):\n",
    "        tuple = (count, movies_popularity_dict[index_to_movie_list[i]], index_to_movie_list[i])\n",
    "        count_pop_tuple_list.append(tuple)\n",
    "    final_sorted_tuple_list = sorted(count_pop_tuple_list, key = lambda tuple: (tuple[0], tuple[1])) \n",
    "    output_movies = []\n",
    "    iter = 0\n",
    "    for tuple in reversed(final_sorted_tuple_list):\n",
    "        iter += 1\n",
    "        if iter <= 5:\n",
    "            output_movies.append(tuple[2])\n",
    "        else:\n",
    "            break\n",
    "    return output_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Dark Knight', 'The Silence of the Lambs', 'The Departed', 'Scarface', 'The Dark Knight Rises']\n",
      "['Forrest Gump', 'Moonrise Kingdom', 'The Terminator', 'Groundhog Day', 'E.T. the Extra-Terrestrial']\n",
      "['The Dark Knight', 'Inglourious Basterds', 'Scarface', 'The Dark Knight Rises', 'Captain Phillips']\n",
      "['The Dark Knight', 'The Silence of the Lambs', 'The Departed', 'Scarface', 'The Dark Knight Rises']\n"
     ]
    }
   ],
   "source": [
    "for out in output_genres:\n",
    "    output_movies = genres_to_top_k_movies(out, k=5)\n",
    "    print(output_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
